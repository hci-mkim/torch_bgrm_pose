{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False)\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\min\\Desktop\\MIN\\Projects\\pytorch_pose_estimation_torchvision\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.mp4', 'video1.avi', 'video2.avi', '동영상을  넣으세요..txt']\n"
     ]
    }
   ],
   "source": [
    "# Video to Image\n",
    "path = \"./img/1_input_video/\"\n",
    "file_list = os.listdir(path)\n",
    "print(\"{}\".format(file_list))\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    vidcap = cv2.VideoCapture(path+file)\n",
    "\n",
    "    def getFrame(sec):\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "        hasFrames,image = vidcap.read()\n",
    "\n",
    "        if hasFrames:\n",
    "            h = 256\n",
    "            w = 256\n",
    "            resized_image = cv2.resize(image, (h, w)).astype(np.float32)\n",
    "            cv2.imwrite(\"./img/2_input/\"+str(count)+\".jpg\", resized_image)     # save frame as JPG file\n",
    "\n",
    "        return hasFrames\n",
    "\n",
    "\n",
    "    sec = 0\n",
    "    frameRate = 1 #//it will capture image in each 1 second\n",
    "    count=1\n",
    "    success = getFrame(sec)\n",
    "    while success:\n",
    "        count = count + 1\n",
    "        sec = sec + frameRate\n",
    "        sec = round(sec, 2)\n",
    "        success = getFrame(sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
    "model = torchvision.models.segmentation.fcn_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.jpg', '100.jpg', '101.jpg', '11.jpg', '12.jpg', '13.jpg', '19.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '31.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '85.jpg', '86.jpg', '87.jpg', '88.jpg', '89.jpg', '9.jpg', '90.jpg', '91.jpg', '92.jpg', '93.jpg', '94.jpg', '95.jpg', '96.jpg', '97.jpg', '98.jpg', '99.jpg']\n",
      "executed in 1.706s\n",
      "executed in 1.615s\n",
      "executed in 1.654s\n",
      "executed in 1.655s\n",
      "executed in 1.748s\n",
      "executed in 1.620s\n",
      "executed in 1.613s\n",
      "executed in 1.617s\n",
      "executed in 1.609s\n",
      "executed in 1.621s\n",
      "executed in 1.608s\n",
      "executed in 1.631s\n",
      "executed in 1.617s\n",
      "executed in 1.630s\n",
      "executed in 1.620s\n",
      "executed in 1.627s\n",
      "executed in 1.610s\n",
      "executed in 1.598s\n",
      "executed in 1.599s\n",
      "executed in 1.605s\n",
      "executed in 1.609s\n",
      "executed in 1.600s\n",
      "executed in 1.615s\n",
      "executed in 1.611s\n",
      "executed in 1.609s\n",
      "executed in 1.614s\n",
      "executed in 1.612s\n",
      "executed in 1.610s\n",
      "executed in 1.631s\n",
      "executed in 1.627s\n",
      "executed in 1.616s\n",
      "executed in 1.613s\n",
      "executed in 1.620s\n",
      "executed in 1.601s\n"
     ]
    }
   ],
   "source": [
    "path = \"./img/2_input/\"\n",
    "file_list = os.listdir(path)\n",
    "print(\"{}\".format(file_list))\n",
    "\n",
    "for file in file_list:\n",
    "    j = 1\n",
    "    image = Image.open(path+file)\n",
    "    \n",
    "    t = time.time()\n",
    "    image = image.resize((256, 256))\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image)\n",
    "\n",
    "    output = model(image_tensor.unsqueeze(0))\n",
    "    print('executed in %.3fs' % (time.time() - t))\n",
    "    \n",
    "    outputs = output['out'].numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "\n",
    "    pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
    "    pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
    "\n",
    "#     label_names = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'] \n",
    "    def label_img_to_color(img):\n",
    "        label_to_color = {\n",
    "            0: [256, 256, 256], 1: [256, 256, 256], 2: [256, 256, 256], 3: [256, 256, 256],\n",
    "            4: [256, 256, 256], 5: [256, 256, 256], 6: [256, 256, 256], 7: [256, 256, 256],\n",
    "            8: [256, 256, 256], 9: [256, 256, 256], 10: [256, 256, 256], 11:[256, 256, 256],\n",
    "            12: [256, 256, 256], 13: [256, 256, 256], 14: [256, 256, 256], 15: [  1, 1,1],\n",
    "            16: [256, 256, 256], 17: [256, 256, 256], 18: [256, 256, 256], 19: [256, 256, 256]\n",
    "            }\n",
    "\n",
    "        img_height, img_width = img.shape\n",
    "        img_color = np.zeros((img_height, img_width, 3))\n",
    "        for row in range(img_height):\n",
    "            for col in range(img_width):\n",
    "                label = img[row, col]\n",
    "                img_color[row, col] = np.array(label_to_color[label])\n",
    "\n",
    "        return img_color\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
    "    img = image_tensor #imgs[i] # (shape: (3, img_h, img_w))\n",
    "    \n",
    "    # Image Tensor to Numpy\n",
    "    img = img.data.cpu().numpy()\n",
    "    img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
    "\n",
    "    # Numpy Image Processing\n",
    "    pred_label_img_color = label_img_to_color(pred_label_img)\n",
    "    overlayed_img = img * pred_label_img_color\n",
    "    overlayed_img = overlayed_img * 255 + img\n",
    "    overlayed_img = overlayed_img[..., ::-1]\n",
    "    \n",
    "    # Image Save\n",
    "    cv2.imwrite('./img/3_output_bg/bg_rm_'+str(file) , overlayed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
